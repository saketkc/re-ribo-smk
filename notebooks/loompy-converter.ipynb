{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmb-panasas2/skchoudh/software_frozen/anaconda27/envs/riboraptor/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['sample']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import loompy\n",
    "from riboraptor.helpers import path_leaf, mkdir_p\n",
    "import glob\n",
    "from linecache import getline\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_translating_ORFs(root_dir='/staging/as/skchoudh/re-ribo-analysis/hg38', ribocop_dir_name='ribocop_results_Feb2019_longest'):\n",
    "    #root_dir ='/staging/as/skchoudh/re-ribo-analysis/hg38'\n",
    "    orf_tsv_list = glob.glob('{}/*/{}/*_translating_ORFs.tsv'.format(root_dir, ribocop_dir_name))\n",
    "    srx = [path_leaf(filepath).replace('_translating_ORFs.tsv', '') for filepath in orf_tsv_list]\n",
    "    srp = [filepath.split(ribocop_dir_name)[0].split('/')[-2] for filepath in orf_tsv_list]\n",
    "    #df = []\n",
    "    #for filepath in tqdm(orf_tsv_list):\n",
    "    #    df += [pd.read_table(filepath, usecols=['ORF_ID', 'profile']).set_index('ORF_ID')]\n",
    "    return list(zip(srp, srx, orf_tsv_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = find_translating_ORFs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_filepath = '/home/cmb-06/as/skchoudh/genomes/hg38/ribocop_hg38_feb2019_annotation_longest_candidate_orfs.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ENST00000626826.1     2787\n",
       "ENST00000597346.1     1225\n",
       "ENST00000623130.1     1181\n",
       "ENST00000623959.1      746\n",
       "ENST00000623111.1      734\n",
       "ENST00000624945.1      684\n",
       "ENST00000604411.1      524\n",
       "ENST00000458178.2      500\n",
       "ENST00000262160.10     451\n",
       "ENST00000330753.5      405\n",
       "ENST00000369851.5      362\n",
       "ENST00000609686.3      351\n",
       "ENST00000558190.5      343\n",
       "ENST00000614987.4      343\n",
       "ENST00000357258.8      337\n",
       "ENST00000501122.2      330\n",
       "ENST00000377861.4      319\n",
       "ENST00000623075.1      314\n",
       "ENST00000552951.6      313\n",
       "ENST00000295851.9      296\n",
       "ENST00000355086.7      295\n",
       "ENST00000611864.4      292\n",
       "ENST00000623543.1      285\n",
       "ENST00000624072.1      281\n",
       "ENST00000379887.8      279\n",
       "ENST00000429829.5      278\n",
       "ENST00000394480.6      276\n",
       "ENST00000315930.10     272\n",
       "ENST00000623726.1      272\n",
       "ENST00000607772.5      270\n",
       "                      ... \n",
       "ENST00000523348.1        1\n",
       "ENST00000636436.1        1\n",
       "ENST00000589642.5        1\n",
       "ENST00000592026.1        1\n",
       "ENST00000555614.5        1\n",
       "ENST00000415943.6        1\n",
       "ENST00000328064.2        1\n",
       "ENST00000508122.5        1\n",
       "ENST00000582767.1        1\n",
       "ENST00000385290.1        1\n",
       "ENST00000429746.1        1\n",
       "ENST00000579892.1        1\n",
       "ENST00000424740.5        1\n",
       "ENST00000542154.5        1\n",
       "ENST00000262970.9        1\n",
       "ENST00000542207.5        1\n",
       "ENST00000535476.6        1\n",
       "ENST00000327216.3        1\n",
       "ENST00000551642.1        1\n",
       "ENST00000562580.5        1\n",
       "ENST00000504104.1        1\n",
       "ENST00000365118.2        1\n",
       "ENST00000593005.5        1\n",
       "ENST00000604508.1        1\n",
       "ENST00000590189.5        1\n",
       "ENST00000614213.1        1\n",
       "ENST00000437875.1        1\n",
       "ENST00000384053.1        1\n",
       "ENST00000505894.2        1\n",
       "ENST00000525080.1        1\n",
       "Name: transcript_id, Length: 192961, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation = pd.read_table('/home/cmb-06/as/skchoudh/genomes/hg38/ribocop_hg38_feb2019_annotation_longest_candidate_orfs.tsv')\n",
    "annotation.transcript_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192961"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotation.transcript_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORF_ID</th>\n",
       "      <th>ORF_type</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>transcript_type</th>\n",
       "      <th>gene_id</th>\n",
       "      <th>gene_name</th>\n",
       "      <th>gene_type</th>\n",
       "      <th>chrom</th>\n",
       "      <th>strand</th>\n",
       "      <th>start_codon</th>\n",
       "      <th>coordinate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENST00000335137.3_69091_70005_915</td>\n",
       "      <td>annotated</td>\n",
       "      <td>ENST00000335137.3</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>ENSG00000186092.4</td>\n",
       "      <td>OR4F5</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>chr1</td>\n",
       "      <td>+</td>\n",
       "      <td>ATG</td>\n",
       "      <td>69091-70005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENST00000624431.1_182709_184155_399</td>\n",
       "      <td>annotated</td>\n",
       "      <td>ENST00000624431.1</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>ENSG00000279928.1</td>\n",
       "      <td>FO538757.2</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>chr1</td>\n",
       "      <td>+</td>\n",
       "      <td>ATG</td>\n",
       "      <td>182709-182746,183114-183240,183922-184155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENST00000623834.3_185220_195411_1077</td>\n",
       "      <td>annotated</td>\n",
       "      <td>ENST00000623834.3</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>ENSG00000279457.3</td>\n",
       "      <td>FO538757.1</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>chr1</td>\n",
       "      <td>-</td>\n",
       "      <td>ATG</td>\n",
       "      <td>185220-185350,185491-185559,187376-187577,1877...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENST00000623083.3_185220_195411_1392</td>\n",
       "      <td>annotated</td>\n",
       "      <td>ENST00000623083.3</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>ENSG00000279457.3</td>\n",
       "      <td>FO538757.1</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>chr1</td>\n",
       "      <td>-</td>\n",
       "      <td>ATG</td>\n",
       "      <td>185220-185350,185491-185559,186317-186469,1871...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENST00000624735.1_184930_200086_1365</td>\n",
       "      <td>annotated</td>\n",
       "      <td>ENST00000624735.1</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>ENSG00000279457.3</td>\n",
       "      <td>FO538757.1</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>chr1</td>\n",
       "      <td>-</td>\n",
       "      <td>ATG</td>\n",
       "      <td>184930-184971,184977-185049,185529-185559,1863...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ORF_ID   ORF_type      transcript_id  \\\n",
       "0     ENST00000335137.3_69091_70005_915  annotated  ENST00000335137.3   \n",
       "1   ENST00000624431.1_182709_184155_399  annotated  ENST00000624431.1   \n",
       "2  ENST00000623834.3_185220_195411_1077  annotated  ENST00000623834.3   \n",
       "3  ENST00000623083.3_185220_195411_1392  annotated  ENST00000623083.3   \n",
       "4  ENST00000624735.1_184930_200086_1365  annotated  ENST00000624735.1   \n",
       "\n",
       "  transcript_type            gene_id   gene_name       gene_type chrom strand  \\\n",
       "0  protein_coding  ENSG00000186092.4       OR4F5  protein_coding  chr1      +   \n",
       "1  protein_coding  ENSG00000279928.1  FO538757.2  protein_coding  chr1      +   \n",
       "2  protein_coding  ENSG00000279457.3  FO538757.1  protein_coding  chr1      -   \n",
       "3  protein_coding  ENSG00000279457.3  FO538757.1  protein_coding  chr1      -   \n",
       "4  protein_coding  ENSG00000279457.3  FO538757.1  protein_coding  chr1      -   \n",
       "\n",
       "  start_codon                                         coordinate  \n",
       "0         ATG                                        69091-70005  \n",
       "1         ATG          182709-182746,183114-183240,183922-184155  \n",
       "2         ATG  185220-185350,185491-185559,187376-187577,1877...  \n",
       "3         ATG  185220-185350,185491-185559,186317-186469,1871...  \n",
       "4         ATG  184930-184971,184977-185049,185529-185559,1863...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_for_annotation_row(row):\n",
    "    coordinates = row['coordinate'].split(',')\n",
    "    index = []\n",
    "    for coordinate in coordinates:\n",
    "        coordinate_start, coordinate_end = [int(x) for x in coordinate.split('-')]\n",
    "        index+= list(range(coordinate_start, coordinate_end+1))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy\n",
    "\n",
    "We will create different [loom](http://linnarssonlab.org/loompy/index.html) file for each ORF ID.\n",
    "The problem however is, we have 2397475 ~ 2.5M ORF ids, so we'll have to have so may files.\n",
    "However, we'll try to minimize the organization burden by having separating everything by transcript ID.\n",
    "Still need 2.5M individual transcript ID files. \n",
    "\n",
    "## Potential issues.\n",
    "\n",
    "1. If the user comes to us with their own query, there's no way for us to not force them to use\n",
    "RiboCop, or at least somehow get this through querying the bam. But what easy way is out there anyway?\n",
    "\n",
    "2. This ORF list is conservative. Ideally we have more than 10M ORFs. But probably not feasible to\n",
    "support them all.\n",
    "\n",
    "So we should make peace with these two limitations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘outfile.loom’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! rm outfile.loom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2787 [00:00<?, ?it/s]\n",
      "  0%|          | 0/131 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/131 [00:05<12:34,  5.80s/it]\u001b[A\n",
      "  2%|▏         | 2/131 [00:12<12:46,  5.94s/it]\u001b[A\n",
      "  2%|▏         | 3/131 [00:15<11:15,  5.28s/it]\u001b[A\n",
      "  3%|▎         | 4/131 [00:19<10:14,  4.84s/it]\u001b[A\n",
      "  4%|▍         | 5/131 [00:23<09:30,  4.53s/it]\u001b[A\n",
      "  5%|▍         | 6/131 [00:27<08:56,  4.29s/it]\u001b[A\n",
      "  5%|▌         | 7/131 [00:30<08:33,  4.14s/it]\u001b[A\n",
      "  6%|▌         | 8/131 [00:34<08:17,  4.04s/it]\u001b[A\n",
      "  7%|▋         | 9/131 [00:38<08:01,  3.95s/it]\u001b[A\n",
      "  8%|▊         | 10/131 [00:42<07:50,  3.88s/it]\u001b[A\n",
      "  8%|▊         | 11/131 [00:45<07:41,  3.85s/it]\u001b[A\n",
      "  9%|▉         | 12/131 [00:49<07:29,  3.78s/it]\u001b[A\n",
      " 10%|▉         | 13/131 [00:53<07:22,  3.75s/it]\u001b[A\n",
      " 11%|█         | 14/131 [00:57<07:19,  3.75s/it]\u001b[A\n",
      " 11%|█▏        | 15/131 [01:00<07:18,  3.78s/it]\u001b[A\n",
      " 12%|█▏        | 16/131 [01:04<07:10,  3.75s/it]\u001b[A\n",
      " 13%|█▎        | 17/131 [01:08<07:00,  3.69s/it]\u001b[A\n",
      " 14%|█▎        | 18/131 [01:11<06:54,  3.67s/it]\u001b[A\n",
      " 15%|█▍        | 19/131 [01:15<06:53,  3.69s/it]\u001b[A\n",
      " 15%|█▌        | 20/131 [01:19<06:49,  3.69s/it]\u001b[A\n",
      " 16%|█▌        | 21/131 [01:22<06:47,  3.70s/it]\u001b[A\n",
      " 17%|█▋        | 22/131 [01:26<06:44,  3.71s/it]\u001b[A\n",
      " 18%|█▊        | 23/131 [01:30<06:47,  3.77s/it]\u001b[A\n",
      " 18%|█▊        | 24/131 [01:34<06:43,  3.77s/it]\u001b[A\n",
      " 19%|█▉        | 25/131 [01:38<06:40,  3.78s/it]\u001b[A\n",
      " 20%|█▉        | 26/131 [01:41<06:31,  3.73s/it]\u001b[A\n",
      " 21%|██        | 27/131 [01:45<06:28,  3.73s/it]\u001b[A\n",
      " 21%|██▏       | 28/131 [01:49<06:22,  3.71s/it]\u001b[A\n",
      " 22%|██▏       | 29/131 [01:52<06:19,  3.72s/it]\u001b[A\n",
      " 23%|██▎       | 30/131 [02:33<24:52, 14.78s/it]\u001b[A\n",
      " 24%|██▎       | 31/131 [02:37<19:05, 11.45s/it]\u001b[A\n",
      " 24%|██▍       | 32/131 [02:40<15:07,  9.17s/it]\u001b[A\n",
      " 25%|██▌       | 33/131 [02:44<12:19,  7.55s/it]\u001b[A\n",
      " 26%|██▌       | 34/131 [02:48<10:25,  6.45s/it]\u001b[A\n",
      " 27%|██▋       | 35/131 [02:52<09:03,  5.67s/it]\u001b[A\n",
      " 27%|██▋       | 36/131 [02:56<08:05,  5.11s/it]\u001b[A\n",
      " 28%|██▊       | 37/131 [03:00<07:37,  4.87s/it]\u001b[A\n",
      " 29%|██▉       | 38/131 [03:04<07:02,  4.55s/it]\u001b[A\n",
      " 30%|██▉       | 39/131 [03:08<06:37,  4.32s/it]\u001b[A\n",
      " 31%|███       | 40/131 [03:12<06:35,  4.35s/it]\u001b[A\n",
      " 31%|███▏      | 41/131 [03:16<06:16,  4.18s/it]\u001b[A\n",
      " 32%|███▏      | 42/131 [03:20<06:00,  4.06s/it]\u001b[A\n",
      " 33%|███▎      | 43/131 [03:24<05:52,  4.00s/it]\u001b[A\n",
      " 34%|███▎      | 44/131 [03:27<05:42,  3.94s/it]\u001b[A\n",
      " 34%|███▍      | 45/131 [03:32<05:50,  4.07s/it]\u001b[A\n",
      " 35%|███▌      | 46/131 [03:35<05:39,  3.99s/it]\u001b[A\n",
      " 36%|███▌      | 47/131 [03:39<05:32,  3.96s/it]\u001b[A\n",
      " 37%|███▋      | 48/131 [03:43<05:24,  3.91s/it]\u001b[A\n",
      " 37%|███▋      | 49/131 [03:47<05:18,  3.88s/it]\u001b[A\n",
      " 38%|███▊      | 50/131 [03:51<05:12,  3.86s/it]\u001b[A\n",
      " 39%|███▉      | 51/131 [03:55<05:05,  3.82s/it]\u001b[A\n",
      " 40%|███▉      | 52/131 [03:59<05:15,  4.00s/it]\u001b[A\n",
      " 40%|████      | 53/131 [04:03<05:05,  3.91s/it]\u001b[A\n",
      " 41%|████      | 54/131 [04:06<04:58,  3.87s/it]\u001b[A\n",
      " 42%|████▏     | 55/131 [04:10<04:52,  3.85s/it]\u001b[A\n",
      " 43%|████▎     | 56/131 [04:14<04:46,  3.82s/it]\u001b[A\n",
      " 44%|████▎     | 57/131 [04:18<04:37,  3.76s/it]\u001b[A\n",
      " 44%|████▍     | 58/131 [04:21<04:32,  3.74s/it]\u001b[A\n",
      " 45%|████▌     | 59/131 [04:25<04:35,  3.82s/it]\u001b[A\n",
      " 46%|████▌     | 60/131 [04:31<05:18,  4.49s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "transcript_id = 'ENST00000626826.1'\n",
    "orfs_for_txid = annotation[annotation.transcript_id == transcript_id]\n",
    "\n",
    "\"\"\"\n",
    "# Stra\n",
    "#for orf in orfs\n",
    "loompy.create(filename, matrix, row_attrs, col_attrs)\n",
    "with loompy.new(\"outfile.loom\") as dsout:\n",
    "    for sample in samples:\n",
    "        with loompy.connect(sample) as dsin:\n",
    "            logging.info(f\"Appending {sample}.\")\n",
    "            dsout.add_columns(ds.layers, col_attrs=dsin.col_attrs, row_attrs=dsin.row_attrs)\n",
    "\"\"\"\n",
    "out_root_dir = '/staging/as/skchoudh/ribopod_hg38_looms/'\n",
    "for line_number, row in  tqdm(orfs_for_txid.iterrows(), total=orfs_for_txid.shape[0]):\n",
    "    annotation_orf_id = row['ORF_ID']\n",
    "    out_dir = os.path.join(out_root_dir, annotation_orf_id)\n",
    "    mkdir_p(out_dir)\n",
    "    loom_file_path = os.path.join(out_dir, 'loomfile.loom')\n",
    "    orf_index = create_index_for_annotation_row(row)\n",
    "    chrom = row['chrom']\n",
    "    sample = sample_list[0]      \n",
    "    filepath = sample[2]\n",
    "    line = getline(filepath, line_number+2)\n",
    "    line_splitted = line.split('\\t')\n",
    "    file_orf_id = line_splitted[0]\n",
    "    assert file_orf_id == annotation_orf_id\n",
    "    profile = eval(line_splitted[-1])\n",
    "\n",
    "    row_attrs = ['{}_{}'.format(chrom, pos) for pos in orf_index]\n",
    "\n",
    "    row_attrs = { \"chr_pos\": np.array(row_attrs) }\n",
    "    col_attrs = { \"sample\":  np.array([sample[0]])}\n",
    "    profile = np.array([profile]).reshape(len(row_attrs['chr_pos']), 1)\n",
    "    loompy.create(loom_file_path, profile,\n",
    "                  row_attrs, col_attrs)\n",
    "\n",
    "    with loompy.connect(loom_file_path) as dsout:\n",
    "        for sample in tqdm(sample_list[1:]):\n",
    "            filepath = sample[2]\n",
    "            # +2 because the indexing is 1-based and the first \n",
    "            # line is a header\n",
    "            line = getline(filepath, line_number+2)\n",
    "            \"\"\"\n",
    "            ['ORF_ID',\n",
    "            'ORF_type',\n",
    "            'status',\n",
    "            'phase_score',\n",
    "            'read_count',\n",
    "            'length',\n",
    "            'valid_codons',\n",
    "            'transcript_id',\n",
    "            'transcript_type',\n",
    "            'gene_id',\n",
    "            'gene_name',\n",
    "            'gene_type',\n",
    "            'chrom',\n",
    "            'strand',\n",
    "            'start_codon',\n",
    "            'profile']\n",
    "            \"\"\"\n",
    "            line_splitted = line.split('\\t')\n",
    "            file_orf_id = line_splitted[0]\n",
    "            assert file_orf_id == annotation_orf_id\n",
    "            profile = eval(line_splitted[-1])\n",
    "            col_attrs = { \"sample\":  np.array([sample[0]])}\n",
    "            profile = np.array([profile]).reshape(len(row_attrs['chr_pos']), 1)\n",
    "\n",
    "            dsout.add_columns(profile, col_attrs=col_attrs)# row_attrs=row_attrs)\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_id = 'ENST00000626826.1'\n",
    "orfs_for_txid = annotation[annotation.transcript_id == transcript_id]\n",
    "\n",
    "\"\"\"\n",
    "# Stra\n",
    "#for orf in orfs\n",
    "loompy.create(filename, matrix, row_attrs, col_attrs)\n",
    "with loompy.new(\"outfile.loom\") as dsout:\n",
    "    for sample in samples:\n",
    "        with loompy.connect(sample) as dsin:\n",
    "            logging.info(f\"Appending {sample}.\")\n",
    "            dsout.add_columns(ds.layers, col_attrs=dsin.col_attrs, row_attrs=dsin.row_attrs)\n",
    "\"\"\"\n",
    "out_root_dir = '/staging/as/skchoudh/ribopod_hg38_looms/'\n",
    "for sample_index, sample in enumerate(tqdm(sample_list)):\n",
    "    df = pd.read_table(sample[2], usecols=['ORF_ID', 'profile'])\n",
    "    for orf_id\n",
    "    df.apply()\n",
    "    \n",
    "    annotation_orf_id = row['ORF_ID']\n",
    "    out_dir = os.path.join(out_root_dir, annotation_orf_id)\n",
    "    mkdir_p(out_dir)\n",
    "    loom_file_path = os.path.join(out_dir, 'loomfile.loom')\n",
    "    orf_index = create_index_for_annotation_row(row)\n",
    "    chrom = row['chrom']\n",
    "    sample = sample_list[0]      \n",
    "    filepath = sample[2]\n",
    "    line = getline(filepath, line_number+2)\n",
    "    line_splitted = line.split('\\t')\n",
    "    file_orf_id = line_splitted[0]\n",
    "    assert file_orf_id == annotation_orf_id\n",
    "    profile = eval(line_splitted[-1])\n",
    "\n",
    "    row_attrs = ['{}_{}'.format(chrom, pos) for pos in orf_index]\n",
    "\n",
    "    row_attrs = { \"chr_pos\": np.array(row_attrs) }\n",
    "    col_attrs = { \"sample\":  np.array([sample[0]])}\n",
    "    profile = np.array([profile]).reshape(len(row_attrs['chr_pos']), 1)\n",
    "    loompy.create(loom_file_path, profile,\n",
    "                  row_attrs, col_attrs)\n",
    "\n",
    "    with loompy.connect(loom_file_path) as dsout:\n",
    "        for sample in tqdm(sample_list[1:]):\n",
    "            filepath = sample[2]\n",
    "            # +2 because the indexing is 1-based and the first \n",
    "            # line is a header\n",
    "            line = getline(filepath, line_number+2)\n",
    "            \"\"\"\n",
    "            ['ORF_ID',\n",
    "            'ORF_type',\n",
    "            'status',\n",
    "            'phase_score',\n",
    "            'read_count',\n",
    "            'length',\n",
    "            'valid_codons',\n",
    "            'transcript_id',\n",
    "            'transcript_type',\n",
    "            'gene_id',\n",
    "            'gene_name',\n",
    "            'gene_type',\n",
    "            'chrom',\n",
    "            'strand',\n",
    "            'start_codon',\n",
    "            'profile']\n",
    "            \"\"\"\n",
    "            line_splitted = line.split('\\t')\n",
    "            file_orf_id = line_splitted[0]\n",
    "            assert file_orf_id == annotation_orf_id\n",
    "            profile = eval(line_splitted[-1])\n",
    "            col_attrs = { \"sample\":  np.array([sample[0]])}\n",
    "            profile = np.array([profile]).reshape(len(row_attrs['chr_pos']), 1)\n",
    "\n",
    "            dsout.add_columns(profile, col_attrs=col_attrs)# row_attrs=row_attrs)\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SRP010679', 'SRP010679', 'SRP010679', 'SRP010679', 'SRP010679', 'SRP010679', 'SRP010679', 'SRP010679', 'SRP010679', 'SRP010679', 'SRP010679', 'SRP010679', 'SRP029589', 'SRP029589', 'SRP029589', 'SRP063852', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP098789', 'SRP092068', 'SRP092068', 'SRP092068', 'SRP092068', 'SRP002605', 'SRP002605', 'SRP002605', 'SRP017942', 'SRP017942', 'SRP017942', 'SRP017942', 'SRP031501', 'SRP031501', 'SRP031501', 'SRP031501', 'SRP031501', 'SRP031501', 'SRP038695', 'SRP038695']\n"
     ]
    }
   ],
   "source": [
    "with loompy.connect('/staging/as/skchoudh/ribopod_hg38_looms/ENST00000626826.1_102197843_102197914_72/loomfile.loom') as ds:\n",
    "    print(list(ds.col_attrs['SRP']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=50):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "def read_batch_tsv(sample_list):\n",
    "    dfs = [pd.read_table(tsv,\n",
    "                         usecols=['ORF_ID', 'profile'], \n",
    "                         dtype={'ORF_ID':'str', 'profile': 'str'}, \n",
    "                         memory_map=True) for srp, srx, tsv in sample_list]\n",
    "    srps = [srp for srp, srx, tsv in sample_list]\n",
    "    srxs = [srx for srp, srx, tsv in sample_list]\n",
    "    col_attrs = {\"study\": np.array(srps), \"experiment\": np.array(srxs) }\n",
    "    return dfs, col_attrs\n",
    "\n",
    "def write_loom_file(loom_file_path, matrix, col_attrs, row_attrs=None):\n",
    "    \"\"\"Create/Update loom file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath: string\n",
    "              Path to write loomfile\n",
    "    matrix: array_like\n",
    "            Data matrix\n",
    "    col_attrs: dict\n",
    "               A dict of lists with same length as the columns in matrix\n",
    "    row_attrs: dict\n",
    "               A dict of lists with same length as the rows in matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    # Check if loomfile exists\n",
    "    if os.path.isfile(loom_file_path):\n",
    "        # Read its columns attributes\n",
    "        # to see if the new column_attrs are same as previous\n",
    "        with loompy.connect(loom_file_path) as ds:\n",
    "            assert sorted(list(col_attrs.keys())) == sorted(list(ds.col_attrs.keys()))\n",
    "            # Check if nothing is being added again\n",
    "            for key in col_attrs.keys():\n",
    "                assert len(set(col_attrs[key]).intersection(set(ds.col_attrs[key]))) == 0\n",
    "            \n",
    "        # Add columns\n",
    "        with loompy.connect(loom_file_path) as dsout:\n",
    "            dsout.add_columns(matrix, col_attrs=col_attrs)# row_attrs=row_attrs)\n",
    "    else:\n",
    "        loompy.create(loom_file_path,\n",
    "                      matrix,\n",
    "                      row_attrs=row_attrs,\n",
    "                      col_attrs=col_attrs)\n",
    "\n",
    "def get_row_attrs_from_orf(annotation, orf_id):\n",
    "    orf_row = annotation.loc[annotation.ORF_ID==orf_id].iloc[0]\n",
    "    orf_index = create_index_for_annotation_row(orf_row)\n",
    "    chrom = orf_row['chrom']\n",
    "    row_attrs = ['{}_{}'.format(chrom, pos) for pos in orf_index]\n",
    "    row_attrs = { \"chr_pos\": np.array(row_attrs) }\n",
    "    return row_attrs\n",
    "\n",
    "\n",
    "def write_loom_for_dfs(loom_file_path, list_of_dfs, col_attrs, row_attrs, orf_id):\n",
    "    # Given a list of dfs,\n",
    "    dfs_subset = [df[df.ORF_ID==orf_id].iloc[0] for df in list_of_dfs]\n",
    "    profile_stacked = [literal_eval(df.profile) for df in dfs_subset]\n",
    "    matrix = np.array(profile_stacked).T\n",
    "    write_loom_file(loom_file_path, matrix, col_attrs, row_attrs)\n",
    "\n",
    "    \n",
    "    \n",
    "def write_loom_batches(sample_list, annotation_filepath, batch_size=50):\n",
    "    print('Reading annotation ... ')\n",
    "    annotation = pd.read_table(annotation_filepath)\n",
    "    print('Done!')\n",
    "    out_root_dir = '/staging/as/skchoudh/ribopod_hg38_looms/'\n",
    "\n",
    "    ORF_IDS = annotation.ORF_ID.tolist()\n",
    "    TX_IDS = annotation.transcript_id.tolist()\n",
    "    for batch_sample in batch(sample_list, 5):\n",
    "        # Read all the tsvs in this batch\n",
    "        print('Reading batch tsv ... ')\n",
    "        list_of_dfs, col_attrs = read_batch_tsv(batch_sample)        \n",
    "        print('Done! ')\n",
    "        # For each ORF in all tsvs\n",
    "        # write a loom file\n",
    "        # organized by `transcript_id/ORF_ID.loom`\n",
    "        for tx_id, orf_id in zip(TX_IDS, ORF_IDS):\n",
    "            row_attrs = get_row_attrs_from_orf(annotation, orf_id)            \n",
    "            out_dir = os.path.join(out_root_dir, tx_id)\n",
    "            mkdir_p(out_dir)\n",
    "            loom_file_path = os.path.join(out_dir, '{}.loom'.format(orf_id))\n",
    "            write_loom_for_dfs(loom_file_path, list_of_dfs, col_attrs, row_attrs, orf_id)\n",
    "        del list_of_dfs\n",
    "        del col_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading annotation ... \n",
      "Done!\n",
      "Reading batch tsv ... \n",
      "Done! \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-81c5bc93e9d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_loom_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-8243a8bfa2c2>\u001b[0m in \u001b[0;36mwrite_loom_batches\u001b[0;34m(sample_list, annotation_filepath, batch_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mmkdir_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mloom_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}.loom'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morf_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mwrite_loom_for_dfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloom_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_attrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_attrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morf_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mlist_of_dfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mcol_attrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-8243a8bfa2c2>\u001b[0m in \u001b[0;36mwrite_loom_for_dfs\u001b[0;34m(loom_file_path, list_of_dfs, col_attrs, row_attrs, orf_id)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprofile_stacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfs_subset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_stacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mwrite_loom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloom_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_attrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_attrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-8243a8bfa2c2>\u001b[0m in \u001b[0;36mwrite_loom_file\u001b[0;34m(loom_file_path, matrix, col_attrs, row_attrs)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# Check if nothing is being added again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol_attrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_attrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_attrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Add columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "write_loom_batches(sample_list, annotation_filepath, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.6 s ± 64.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df = pd.read_table('/staging/as/skchoudh/re-ribo-analysis/hg38/SRP010679/ribocop_results_Feb2019_longest/SRX118286_translating_ORFs.tsv',\n",
    "                            usecols=['ORF_ID', 'profile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.3 s ± 61.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "df = pd.read_table('/staging/as/skchoudh/re-ribo-analysis/hg38/SRP010679/ribocop_results_Feb2019_longest/SRX118286_translating_ORFs.tsv',\n",
    "                   usecols=['ORF_ID', 'profile'], \n",
    "                   dtype={'ORF_ID':'str', 'profile': 'str'}, \n",
    "                   memory_map=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit profiles = df.profile.apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LoomConnection' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-932e90ad94f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloom_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/staging/as/skchoudh/ribopod_hg38_looms/ENST00000487959.5/ENST00000487959.5_147844554_147844688_135.loom'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mloompy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloom_file_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'LoomConnection' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "loom_file_path = '/staging/as/skchoudh/ribopod_hg38_looms/ENST00000487959.5/ENST00000487959.5_147844554_147844688_135.loom'\n",
    "with loompy.connect(loom_file_path) as ds:\n",
    "    print(ds.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/staging/as/skchoudh/re-ribo-analysis/hg38/SRP010679/ribocop_results_Feb2019_longest/SRX118286_translating_ORFs.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 67875/2397475 [00:52<26:25, 1469.25it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-9800a1ba4b5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfile_orf_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_splitted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_splitted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_getline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-9800a1ba4b5a>\u001b[0m in \u001b[0;36mtest_getline\u001b[0;34m(filepath, list_of_lines)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mline_splitted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfile_orf_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_splitted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_splitted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest_getline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 67875/2397475 [01:10<26:25, 1469.25it/s]"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "def test_getline(filepath, list_of_lines):\n",
    "    for line_number in tqdm(list_of_lines):\n",
    "        line = getline(filepath, line_number+2)\n",
    "        line_splitted = line.split('\\t')\n",
    "        file_orf_id = line_splitted[0]\n",
    "        profile = eval(line_splitted[-1])\n",
    "test_getline(filepath, range(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:riboraptor]",
   "language": "python",
   "name": "conda-env-riboraptor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
