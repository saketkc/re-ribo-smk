from collections import defaultdict
import os
import errno
import glob
import numpy as np
import pandas as pd
from os.path import join

GTF_VERSION = None
ADAPTER = None
RIBOTRICER_ANNOTATION_PREFIX = None

include: config['config_path']

assert GTF_VERSION is not None
assert RIBOTRICER_ANNOTATION_PREFIX is not None

#RIBOTRICER_ANNOTATION_PREFIX = RIBOTRICER_ANNOTATION.replace("_candidate_orfs.tsv", "")
RIBOTRICER_ANNOTATION = RIBOTRICER_ANNOTATION_PREFIX + "_candidate_orfs.tsv"

# Append GTF in STAR index
if GTF_VERSION not in STAR_INDEX:
  STAR_INDEX = STAR_INDEX + '_' + GTF_VERSION


REGIONS = ['super_uORF,uORF', 'super_uORF,uORF,overlap_uORF', 'super_uORF,overlap_uORF', 'annotated', 'super_dORF']
#REGIONS = ['super_uORF', 'annotated', 'super_dORF']


def mkdir_p(path):
  """Python version mkdir -p

  Parameters
  ----------

  path : str
  """
  if path:
    try:
      os.makedirs(path) 
    except OSError as exc:  # Python >2.5
      if exc.errno == errno.EEXIST and os.path.isdir(path):
        pass
      else:
        raise



mkdir_p(os.path.join(OUT_DIR, 'slurm-logs'))
workdir: OUT_DIR

ALL_SRA_FILES = glob.glob('{}/**/*.sra'.format(RAWDATA_DIR), recursive=True)
SRX_ID = defaultdict(list)

for sample in ALL_SRA_FILES:
    srx, srr = sample.replace('{}'.format(RAWDATA_DIR),'').lstrip('/').rstrip('/').split('/')
    SRX_ID[srx].append(srr.replace('.sra', ''))

SRR_ID = list(SRX_ID.values())
SRX_SAMPLES = list(SRX_ID.keys())

ALL_SRR = [item for sublist in SRR_ID for item in sublist]


def merge_bams_input(wildcards):
    return ['bams_srr/{}.bam'.format(srr) for srr in SRX_ID[wildcards.sample]]


def merge_fastq_input(wildcards):
    return ['sratofastq/{}.fastq.gz'.format(srr) for srr in SRX_ID[wildcards.sample]]


def sra_to_fastq_input(wildcards):
    srr_id = wildcards.sample
    for srx_id in list(SRX_ID.keys()):
        value = SRX_ID[srx_id]
        if srr_id in list(value):
            return ancient(str(join(RAWDATA_DIR, srx_id, srr_id+'.sra')))
    print("WRONG encodeterend: {}".format(srr_id))


def get_wrapper(wrapper_name):
    path = os.path.dirname(os.path.abspath(os.path.realpath(workflow.snakefile)))
    return 'file://' + os.path.join(path,
                                    'wrappers',
                                    wrapper_name + '.py')


def get_multiqc_report_input(wildcards):
    return glob.glob('multiqc_report/multiqc_plots/png/*.png')

rule all:
  input:
    expand('ribotricer_results/{sample}_translating_ORFs.tsv', sample=SRX_SAMPLES), 
    expand('ribotricer_results/region_counts/{region}/{sample}_counts_cnt.txt', region=REGIONS, sample=SRX_SAMPLES),
    'featureCounts_exon/fcounts.tsv',
    'featureCounts_CDS/fcounts.tsv'


def total_genome_size():
    df = pd.read_table(CHROM_SIZES, names=['chrom', 'sizes'])
    total = df['sizes'].sum()
    return total


def get_align_intro_params():
    df = pd.read_table(INTRON_BED, names=['chrom', 'start', 'end', 'name', 'score', 'strand'])
    lengths = df['end'] - df['start']

    ## Based on small genomes. See https://groups.google.com/forum/#!topic/rna-star/hQeHTBbkc0c
    alignintronNmin = max(4, lengths.min())
    alignintronNmax = lengths.max()
    return alignintronNmin, alignintronNmax

ALIGN_INTRON_Nmin, ALIGN_INTRON_Nmax = get_align_intro_params()
TOTAL_GENOME_SIZE = total_genome_size()
## Small genome optimization
## See STAR manual 2.2.5
SA_INDEX_Nbases = int(np.floor(min(14, np.log2(TOTAL_GENOME_SIZE)/2.0-1)))


rule create_index:
    input:
        fasta=GENOME_FASTA,
        gtf=GTF
    output: 
        dir = directory(STAR_INDEX),
        #index = os.path.join(STAR_INDEX, 'SAindex') 
    threads: 16
    shell:
        r'''mkdir -p {output} && STAR --runThreadN {threads} --runMode genomeGenerate --genomeDir {output.dir} --genomeSAindexNbases {SA_INDEX_Nbases} --genomeFastaFiles {input.fasta} --sjdbGTFfile {input.gtf}'''
      # Add this for panTro3
      # --genomeChrBinNbits 16\
      # Add this for Mmul8
      # --genomeChrBinNbits 14\

rule sra_to_fastq:
    input: sra_to_fastq_input
    output: temp('sratofastq/{sample}.fastq.gz')
    threads: 16
    params:
        prefix_sra='sratofastq/{sample}.sra.fastq',
        prefix='sratofastq/{sample}.fastq',
        temp_dir='/tmp/{sample}_sratofastq',
    shell:
        r'''parallel-fastq-dump --threads 16 --outdir sratofastq/ --gzip -s {input}'''

rule perform_trimming:
    input:
        R1 = 'sratofastq/{sample}.fastq.gz'
    output:
        pass1_fq = temp('preprocessed_step1/{sample}_trimmed.fq.gz'),
        pass2_fq = temp('preprocessed/{sample}_trimmed_trimmed.fq.gz'),
        pass1_fq_report = 'preprocessed_step1/{sample}.fastq.gz_trimming_report.txt',
        #pass2_fq_report = 'preprocessed/{sample}_trimmed.fq.gz_trimming_report.txt',
    params:
        pass1_dir = 'preprocessed_step1',
        pass2_dir = 'preprocessed',
        phred_cutoff = 5,
        min_length = 18, # Discard sequences shorter than this length
        max_length = 38, # Discard sequences longer than this length
        adapter = ADAPTER
        # --max_length for SmallRNA: Should let to 37?
        # --illumina: For adapter trimming?
    wrapper:
        get_wrapper('trimming_wrapper')


rule prepare_orfs:
  input: 
    gtf = GTF,
    fasta = GENOME_FASTA
  output: RIBOTRICER_ANNOTATION
  shell:
    r'''ribotricer prepare-orfs --gtf {input.gtf} --prefix {RIBOTRICER_ANNOTATION_PREFIX} --fasta {input.fasta} --start_codons 'ATG,AAG,ACG,ATC,GTG,AGG,ATA,ATT,CTG,TTG' --longest'''


rule map_star:
    input:
        R1 = 'preprocessed/{sample}_trimmed_trimmed.fq.gz',
        index = ancient(STAR_INDEX)
    output:
        bam = temp('bams_srr/{sample}.bam'),
        txbam = temp('bams_srr_tx/{sample}.bam'),
        counts = 'STARcounts/{sample}.counts',
        starlogs = 'starlogs/{sample}Log.final.out'
    params:
        chrom_sizes = CHROM_SIZES,
        intron_bed = INTRON_BED,
        tmp_dir = '/tmp'
    threads: 16
    wrapper:
        get_wrapper('star_wrapper')


rule merge_bams:
    input: merge_bams_input
    output: temp('bams/{sample}.bam')
    params:
        tmp_dir = '/tmp'
    threads: 1
    wrapper:
        get_wrapper('merge_bams_wrapper')


rule extract_uniq_mapping:
    input: 'bams/{sample}.bam'
    output: 'bams_unique/{sample}.bam'
    params:
        tmp_dir = '/tmp'
    threads: 16
    wrapper:
        get_wrapper('uniq_mapping_wrapper')


rule predict_orfs:
  input:
    bam = 'bams_unique/{sample}.bam',
    index = RIBOTRICER_ANNOTATION
  output: 'ribotricer_results/{sample}_translating_ORFs.tsv'
  params:
    prefix = 'ribotricer_results/{sample}',
  shell:
    r'''ribotricer detect-orfs --report_all --bam {input.bam} --prefix {params.prefix} --ribotricer_index {input.index}'''


rule count_orfs:
  input: 'ribotricer_results/{sample}_translating_ORFs.tsv'
  output: 'ribotricer_results/region_counts/{region}/{sample}_counts_cnt.txt'
  params:
    prefix = 'ribotricer_results/region_counts/{region}/{sample}_counts',
    region = '{region}'
  shell:
    r'''ribotricer count-orfs --report_all --ribotricer_index {RIBOTRICER_ANNOTATION} --features {params.region} --detected_orfs {input} --prefix {params.prefix}'''

rule featurecounts:
    input:
        bams = expand('bams_unique/{sample}.bam',
                      sample=SRX_SAMPLES),
        bed = GENE_BED
    output: 'featureCounts_exon/fcounts.tsv'
    params:
        annotation  = GTF
    threads: 16
    wrapper:
        get_wrapper('featurecounts_wrapper')

rule featurecounts_cds:
    input:
        bams = expand('bams_unique/{sample}.bam',
                      sample=SRX_SAMPLES),
        bed = GENE_BED
    output: 'featureCounts_CDS/fcounts.tsv'
    params:
        annotation  = GTF
    threads: 16
    wrapper:
        get_wrapper('featurecounts_cds_wrapper')

